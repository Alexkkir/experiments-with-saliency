{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchmetrics import Accuracy\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import shutil\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set_theme()\n",
    "matplotlib.rcParams['figure.figsize'] = (30, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff3a80fd8d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('mnist/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('mnist/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 3, 3, 1, 1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 6, 3, 1, 1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6, 10)\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=10)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = self.loss(pred, y)\n",
    "        acc = self.accuracy(pred, y)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = self.loss(pred, y)\n",
    "        acc = self.accuracy(pred, y)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Define optimizers and LR schedulers. \"\"\"\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.layers.parameters(), 'lr': 3e-4},\n",
    "        ], weight_decay=3e-4)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='max', \n",
    "            factor=0.2, \n",
    "            patience=5, \n",
    "            verbose=True)\n",
    "            \n",
    "        lr_dict = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val/acc\"\n",
    "        } \n",
    "\n",
    "        return [optimizer], [lr_dict]\n",
    "\n",
    "    # OPTIONAL\n",
    "    def training_epoch_end(self, outputs):\n",
    "        \"\"\"log and display average train loss and accuracy across epoch\"\"\"\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "\n",
    "        self.print(f\"| TRAIN loss: {avg_loss:.2f}, acc: {avg_acc:.2f}\" )\n",
    "\n",
    "        self.log('train/loss', avg_loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log('train/acc', avg_acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "    # OPTIONAL\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"log and display average val loss and accuracy\"\"\"\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "\n",
    "        self.print(f\"[Epoch {self.trainer.current_epoch:3}] VALID loss: {avg_loss:.2f}, acc: {avg_acc:.2f}\", end= \" \")\n",
    "\n",
    "        self.log('val/loss', avg_loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log('val/acc', avg_acc, prog_bar=True, on_epoch=True, on_step=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_date=2023-01-15_15:43'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'_date={lib.today()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train/acc</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁</td></tr><tr><td>val/acc</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train/acc</td><td>0.15495</td></tr><tr><td>train/loss</td><td>2.27823</td></tr><tr><td>trainer/global_step</td><td>937</td></tr><tr><td>val/acc</td><td>0.2003</td></tr><tr><td>val/loss</td><td>2.24086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">save hyperparameters at the end #2</strong> at: <a href=\"https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning/runs/s81gu9kg\" target=\"_blank\">https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning/runs/s81gu9kg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230115_154030-s81gu9kg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee050815139b4b8e9851198f77280e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669346099994677, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexkkir/it/research/experiments-with-saliency/wandb/run-20230115_154232-n7leu2zx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning/runs/n7leu2zx\" target=\"_blank\">save hyperparameters at the end #2</a></strong> to <a href=\"https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning\" target=\"_blank\">https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning/runs/n7leu2zx\" target=\"_blank\">https://wandb.ai/alexkkir/MNIST%20example%20with%20pytorhc%20lightning/runs/n7leu2zx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkkir/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project='MNIST example with pytorhc lightning', \n",
    "    name='save hyperparameters at the end #2', \n",
    "    notes='tried to save hp at end')\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "\n",
    "MyModelCheckpoint = ModelCheckpoint(dirpath='checkpoints/mnist/',\n",
    "                                    filename='{epoch}_{val_srocc:.3f}_{val_plcc:.3f}_{val_loss:.3f}' + f'_date={lib.today()}',\n",
    "                                    monitor='val/acc', \n",
    "                                    mode='max', \n",
    "                                    save_top_k=1,\n",
    "                                    save_weights_only=True,\n",
    "                                    verbose=False)\n",
    "\n",
    "MyEarlyStopping = EarlyStopping(monitor = \"val/acc\",\n",
    "                                mode = \"max\",\n",
    "                                patience = 15,\n",
    "                                verbose = True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=100,\n",
    "    accelerator='cpu',\n",
    "    # devices=[1],\n",
    "    callbacks=[MyEarlyStopping, MyModelCheckpoint],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkkir/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /home/alexkkir/it/research/experiments-with-saliency/checkpoints/mnist exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | layers   | Sequential         | 288   \n",
      "1 | loss     | CrossEntropyLoss   | 0     \n",
      "2 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "288       Trainable params\n",
      "0         Non-trainable params\n",
      "288       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b698e97c5df4e5a88bc25aaafb32d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkkir/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/alexkkir/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0] VALID loss: 2.33, acc: 0.11 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkkir/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77db725be66448fcae47a82dd4b2c560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3c0a577c6e4b5c97bfe19db506e9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/acc improved. New best score: 0.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0] VALID loss: 2.27, acc: 0.20 | TRAIN loss: 2.31, acc: 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkkir/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
