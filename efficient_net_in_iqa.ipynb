{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor, ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import shutil\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set_theme()\n",
    "matplotlib.rcParams['figure.figsize'] = (30, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (384, 512)\n",
    "SAL_RESIZE = (12, 16)\n",
    "DATA_ROOT = Path('iqa')\n",
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQADataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, mode, saliency_path=None, transforms=None):\n",
    "        assert isinstance(images_path, str) or isinstance(images_path, Path)\n",
    "        assert isinstance(labels_path, str) or isinstance(labels_path, Path)\n",
    "        assert saliency_path is None or isinstance(saliency_path, str) or isinstance(saliency_path, Path)\n",
    "        assert mode in ['train', 'valid', 'test', 'all']\n",
    "        assert transforms is None or isinstance(transforms, dict) and np.all([isinstance(t, A.BaseCompose) for t in transforms.values()])\n",
    "\n",
    "        TRAIN_RATIO = 0.7\n",
    "        TRAIN_VALID_RATIO = 0.8\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.saliency_path = saliency_path\n",
    "\n",
    "        self.saliency = True if saliency_path is not None else False\n",
    "\n",
    "        df = pd.read_csv(labels_path).astype('float32', errors='ignore')\n",
    "        available = os.listdir(images_path)\n",
    "        df = df[df.name.isin(available)]\n",
    "\n",
    "        train_size = int(TRAIN_RATIO * len(df))\n",
    "        train_valid_size = int(TRAIN_VALID_RATIO * len(df))\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.df = df.iloc[:train_size]\n",
    "        elif mode == 'valid':\n",
    "            self.df = df.iloc[train_size:train_valid_size]\n",
    "        elif mode == 'test':\n",
    "            self.df = df.iloc[train_valid_size:]\n",
    "        elif mode == 'all':\n",
    "            self.df = df\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.file2suffix_saliency = {Path(file).with_suffix(''): Path(file).suffix for file in os.listdir(saliency_path)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        name, subj_mean, subj_std = self.df.iloc[index]\n",
    "        image = cv2.imread(str(self.images_path / name))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.saliency:\n",
    "            name_clean= Path(name).with_suffix('')\n",
    "            name_sal = str(name_clean) + self.file2suffix_saliency[name_clean]\n",
    "            name_sal = str(self.saliency_path / name_sal)\n",
    "            saliency = cv2.imread(name_sal)\n",
    "            saliency = saliency[..., 0:1]\n",
    "\n",
    "        if self.transforms:\n",
    "            NUM_CHANNELS = 3\n",
    "\n",
    "            # first stage: preprocessing (resize)\n",
    "            image = self.transforms['1_both'](image=image)['image']\n",
    "            if self.saliency:\n",
    "                saliency = self.transforms['1_both'](image=saliency)['image']\n",
    "            image = np.dstack([image, saliency])\n",
    "\n",
    "            # second stage: augmentations (hflip, rotate)\n",
    "            image = self.transforms['2_both'](image=image)['image']\n",
    "\n",
    "            # third stage: postprocessing (normalization, to_tensor)\n",
    "            image, saliency = image[..., :NUM_CHANNELS], image[..., NUM_CHANNELS:]\n",
    "            image = self.transforms['3_images'](image=image)['image']\n",
    "            if self.saliency:\n",
    "                saliency = self.transforms['3_saliency'](image=saliency)['image']\n",
    "        \n",
    "        out = {'image': image, 'name': name, 'subj_mean': subj_mean, 'subj_std': subj_std}\n",
    "        if self.saliency:\n",
    "            out['saliency'] = saliency\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\n",
    "    '1_both': A.Compose([\n",
    "        A.Resize(*IMAGE_SHAPE),\n",
    "    ]),\n",
    "    '2_both': A.Compose([\n",
    "        A.HorizontalFlip()\n",
    "    ]),\n",
    "    '3_images': A.Compose([\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    '3_saliency': A.Compose([\n",
    "        A.Resize(*SAL_RESIZE),\n",
    "        A.ToFloat(max_value=255),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "dataset_train = IQADataset(\n",
    "    images_path=DATA_ROOT / 'koniq10k' / 'images',\n",
    "    labels_path=DATA_ROOT / 'koniq_data.csv', \n",
    "    saliency_path=DATA_ROOT / 'koniq10k' / 'saliency_maps',\n",
    "    mode='train', \n",
    "    transforms=transforms)\n",
    "\n",
    "dataset_valid = IQADataset(\n",
    "    images_path=DATA_ROOT / 'koniq10k' / 'images',\n",
    "    labels_path=DATA_ROOT / 'koniq_data.csv', \n",
    "    saliency_path=DATA_ROOT / 'koniq10k' / 'saliency_maps',\n",
    "    mode='valid', \n",
    "    transforms=transforms)\n",
    "\n",
    "dataset_test_koniq = IQADataset(\n",
    "    images_path=DATA_ROOT / 'koniq10k' / 'images',\n",
    "    labels_path=DATA_ROOT / 'koniq_data.csv', \n",
    "    saliency_path=DATA_ROOT / 'koniq10k' / 'saliency_maps',\n",
    "    mode='test', \n",
    "    transforms=transforms)\n",
    "\n",
    "dataset_test_clive = IQADataset(\n",
    "    images_path=DATA_ROOT / 'CLIVE' / 'images',\n",
    "    labels_path=DATA_ROOT / 'clive_data.csv',\n",
    "    saliency_path=DATA_ROOT / 'CLIVE' / 'saliency_maps',\n",
    "    mode='all', \n",
    "    transforms=transforms)\n",
    "\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=16)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=16, shuffle=False, num_workers=16)\n",
    "loader_test_koniq = DataLoader(dataset_test_koniq, batch_size=16, shuffle=False, num_workers=16)\n",
    "loader_test_clive = DataLoader(dataset_test_clive, batch_size=16, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader_train))\n",
    "lib.display_batch(batch, 'subj_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader_valid))\n",
    "lib.display_batch(batch, 'subj_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader_test_koniq))\n",
    "lib.display_batch(batch, 'subj_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader_test_clive))\n",
    "lib.display_batch(batch, 'subj_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout, activation=True):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        if activation:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.BatchNorm1d(out_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, saliency_flg, alpha_sal=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = torchvision.models.efficientnet_b2(weights=torchvision.models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "        backbone = list(backbone.children())[:-2]\n",
    "        backbone = nn.Sequential(*backbone)\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            LinearBlock(1408, 1024, 0.25),\n",
    "            LinearBlock(1024, 256, 0.25),\n",
    "            LinearBlock(256, 1, 0, activation=False)\n",
    "        )\n",
    "\n",
    "        self.sal_conv = nn.Conv2d(1408, 1, (1, 1), 1, 0)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.alpha_sal = alpha_sal if saliency_flg is True else 0\n",
    "        self.saliency_flg = saliency_flg\n",
    "\n",
    "    def saliency_loss(self, pred, y):\n",
    "        pred = pred / pred.mean()\n",
    "        y = y / y.mean()\n",
    "        return ((pred - y) ** 2).mean()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if self.saliency_flg:\n",
    "            saliency = self.sal_conv(x)\n",
    "            x = saliency * x # fusion\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        if self.saliency_flg:\n",
    "            return x, saliency\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"the full training loop\"\"\"\n",
    "        x, sal_target, y = batch['image'], batch['saliency'], batch['subj_mean']\n",
    "\n",
    "        if self.saliency_flg:\n",
    "            pred, sal_pred = self(x)\n",
    "        else:\n",
    "            pred = self(x)\n",
    "        pred = pred.flatten()\n",
    "\n",
    "        loss = self.mse_loss(pred, y) * (1 - self.alpha_sal)\n",
    "        if self.saliency_flg:\n",
    "            loss += self.saliency_loss(sal_pred, sal_target) * self.alpha_sal\n",
    "\n",
    "        true = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        return {'loss': loss, 'results': (true, pred)}\n",
    "    \n",
    "    # OPTIONAL\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"the full validation loop\"\"\"\n",
    "        x, sal_target, y = batch['image'], batch['saliency'], batch['subj_mean']\n",
    "\n",
    "        if self.saliency_flg:\n",
    "            pred, sal_pred = self(x)\n",
    "        else:\n",
    "            pred = self(x)\n",
    "        pred = pred.flatten()\n",
    "\n",
    "        loss = self.mse_loss(pred, y) * (1 - self.alpha_sal)\n",
    "        if self.saliency_flg:\n",
    "            loss += self.saliency_loss(sal_pred, sal_target) * self.alpha_sal\n",
    "\n",
    "        true = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        return {'loss': loss, 'results': (true, pred)}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Define optimizers and LR schedulers. \"\"\"\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.backbone.parameters(), 'lr': 3e-5},\n",
    "            {'params': self.mlp.parameters(), 'lr': 3e-4}\n",
    "        ], weight_decay=3e-4)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='max', \n",
    "            factor=0.2, \n",
    "            patience=5, \n",
    "            verbose=True)\n",
    "            \n",
    "        lr_dict = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_srocc\"\n",
    "        } \n",
    "\n",
    "        return [optimizer], [lr_dict]\n",
    "\n",
    "    # OPTIONAL\n",
    "    def training_epoch_end(self, outputs):\n",
    "        \"\"\"log and display average train loss and accuracy across epoch\"\"\"\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        true = np.concatenate([x['results'][0] for x in outputs])\n",
    "        predicted = np.concatenate([x['results'][1] for x in outputs])\n",
    "\n",
    "        plcc = stats.pearsonr(predicted, true)[0]\n",
    "        srocc = stats.spearmanr(predicted, true)[0]\n",
    "\n",
    "        self.print(f\"| TRAIN plcc: {plcc:.2f}, srocc: {srocc:.2f}, loss: {avg_loss:.2f}\" )\n",
    "\n",
    "        self.log('train_loss', avg_loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log('train_plcc', plcc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log('train_srocc', srocc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "    # OPTIONAL\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"log and display average val loss and accuracy\"\"\"\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        true = np.concatenate([x['results'][0] for x in outputs])\n",
    "        predicted = np.concatenate([x['results'][1] for x in outputs])\n",
    "        \n",
    "        plcc = stats.pearsonr(predicted, true)[0]\n",
    "        srocc = stats.spearmanr(predicted, true)[0]\n",
    "\n",
    "        self.print(f\"[Epoch {self.trainer.current_epoch:3}] VALID plcc: {plcc:.2f}, srocc: {srocc:.2f}, loss: {avg_loss:.2f}\", end= \" \")\n",
    "\n",
    "        self.log('val_loss', avg_loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log('val_plcc', plcc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        self.log('val_srocc', srocc, prog_bar=True, on_epoch=True, on_step=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project='IQA', \n",
    "    name='with saliency',\n",
    "    notes='want to repeat experiments several times to get mean result',\n",
    "    config={\n",
    "        'training_process': 'reduce on platueau by 0.2 + early stopping',\n",
    "        'batch_size': 16,\n",
    "        'model': 'efficient_b2',\n",
    "        'fusion': 'multiplicative',\n",
    "        'datasets': ['KonIQ-10k', 'CLIVE'],\n",
    "        'train/valid/test split on KonIQ': (0.7, 0.1, 0.2)\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "\n",
    "MyModelCheckpoint = ModelCheckpoint(dirpath='checkpoints/',\n",
    "                                    filename=f'date={lib.today()}_' + '{val_srocc:.3f}_{epoch}',\n",
    "                                    monitor='val_srocc', \n",
    "                                    mode='max', \n",
    "                                    save_top_k=1,\n",
    "                                    save_weights_only=True,\n",
    "                                    verbose=False)\n",
    "\n",
    "MyEarlyStopping = EarlyStopping(monitor = \"val_srocc\", \n",
    "                                mode = \"max\",\n",
    "                                patience = 15,\n",
    "                                verbose = True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=100,\n",
    "    accelerator='cpu',\n",
    "    # devices=[1],\n",
    "    callbacks=[MyEarlyStopping, MyModelCheckpoint],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "model = Model(saliency_flg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, loader_train, loader_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model, loader_test_clive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "869e465c4f25397806eb98effb5307a5d5d93b3ae0a2a52b8d6469552bf70b42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
